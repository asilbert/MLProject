{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5bd074-5201-489e-9a6f-1417bbe37434",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder, FunctionTransformer, LabelEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestRegressor, HistGradientBoostingRegressor\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold, train_test_split, KFold, cross_val_score\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet, RidgeCV, LassoCV, ElasticNetCV\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import statsmodels.api as sm\n",
    "from sklearn.svm import SVR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4205f1cc-2976-41c0-ad4e-64e525d2d72a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ames = pd.read_csv('Ames_HousePrice.csv', index_col=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1035e792-b02e-489a-8a02-761a9ff41e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset shape\n",
    "print(Ames.shape)\n",
    "rows, columns = Ames.shape\n",
    "print(f\"The dataset comprises {rows} properties described across {columns} attributes.\")\n",
    "# Display the first few rows of the dataset\n",
    "print(\"First few rows of the dataset:\")\n",
    "print(Ames.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2686f1bb-aef3-4c55-82f9-20de1a0c6d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the data type for each feature\n",
    "data_types = Ames.dtypes\n",
    "# Tally the total by data type\n",
    "type_counts = data_types.value_counts()\n",
    "print(type_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90222ef-3995-4c4d-b4f8-7b15ca3bc253",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the data type for each feature\n",
    "data_types = Ames.dtypes\n",
    "# View a few datatypes from the dataset (first and last 5 features)\n",
    "print(data_types)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf059b5-07e5-4f9d-840c-8dbd62121ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the structure of the dataset\n",
    "print(\"\\nStructure of the dataset:\")\n",
    "print(Ames.info())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27474009-3862-4699-b716-425686469c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the percentage of missing values for each column\n",
    "missing_data = Ames.isnull().sum()\n",
    "missing_percentage = (missing_data / len(Ames)) * 100\n",
    "# Combining the counts and percentages into a DataFrame for better visualization\n",
    "missing_info = pd.DataFrame({'Missing Values': missing_data, 'Percentage': missing_percentage})\n",
    "# Sorting the DataFrame by the percentage of missing values in descending order\n",
    "missing_info = missing_info.sort_values(by='Percentage', ascending=False)\n",
    "# Display columns with missing values\n",
    "print(missing_info[missing_info['Missing Values'] > 0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "badbe84e-cc2c-4880-b805-97d414b202c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import missingno as msno\n",
    "import matplotlib.pyplot as plt\n",
    "msno.matrix(Ames, sparkline=False, fontsize=20)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd53c7d-0396-4002-b4f5-53cc1a81cc33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the percentage of missing values for each column\n",
    "missing_data = Ames.isnull().sum()\n",
    "missing_percentage = (missing_data / len(Ames)) * 100\n",
    "# Combining the counts and percentages into a DataFrame for better visualization\n",
    "missing_info = pd.DataFrame({'Missing Values': missing_data, 'Percentage': missing_percentage})\n",
    "# Sort the DataFrame columns by the percentage of missing values\n",
    "sorted_df = Ames[missing_info.sort_values(by='Percentage', ascending=False).index]\n",
    "# Select the top 15 columns with the most missing values\n",
    "top_15_missing = sorted_df.iloc[:, :15]\n",
    "#Visual with missingno\n",
    "msno.bar(top_15_missing)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d2c9f2-6bce-4a42-beb0-fdc1e84cc929",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "# Filter to show only the top 15 columns with the most missing values\n",
    "top_15_missing_info = missing_info.nlargest(15, 'Percentage')\n",
    "# Create the horizontal bar plot using seaborn\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(x='Percentage', y=top_15_missing_info.index, data=top_15_missing_info, orient='h')\n",
    "plt.title('Top 15 Features with Missing Percentages', fontsize=20)\n",
    "plt.xlabel('Percentage of Missing Values', fontsize=16)\n",
    "plt.ylabel('Features', fontsize=16)\n",
    "plt.yticks(fontsize=11)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b85b402-8941-41f1-b5e0-7e00d1f042b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the below numeric features to categorical features\n",
    "Ames['MSSubClass'] = Ames['MSSubClass'].astype('object')\n",
    "Ames['YrSold'] = Ames['YrSold'].astype('object')\n",
    "Ames['MoSold'] = Ames['MoSold'].astype('object')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c64d64d-5119-4f03-bb71-43a44cec21b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exclude 'PID' and 'SalePrice' from features and specifically handle the 'Electrical' column\n",
    "numeric_features = Ames.select_dtypes(include=['int64', 'float64']).drop(columns=['PID', 'SalePrice']).columns\n",
    "categorical_features = Ames.select_dtypes(include=['object']).columns.difference(['Electrical'])\n",
    "electrical_feature = ['Electrical']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ef0065-c85d-4db9-9e07-aeb7a1864c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually specify the categories for ordinal encoding according to the data dictionary\n",
    "ordinal_order = {\n",
    "    'Electrical': ['Mix', 'FuseP', 'FuseF', 'FuseA', 'SBrkr'],  # Electrical system\n",
    "    'LotShape': ['IR3', 'IR2', 'IR1', 'Reg'],  # General shape of property\n",
    "    'Utilities': ['ELO', 'NoSeWa', 'NoSewr', 'AllPub'],  # Type of utilities available\n",
    "    'LandSlope': ['Sev', 'Mod', 'Gtl'],  # Slope of property\n",
    "    'ExterQual': ['Po', 'Fa', 'TA', 'Gd', 'Ex'],  # Evaluates the quality of the material on the exterior\n",
    "    'ExterCond': ['Po', 'Fa', 'TA', 'Gd', 'Ex'],  # Evaluates the present condition of the material on the exterior\n",
    "    'BsmtQual': ['None', 'Po', 'Fa', 'TA', 'Gd', 'Ex'],  # Height of the basement\n",
    "    'BsmtCond': ['None', 'Po', 'Fa', 'TA', 'Gd', 'Ex'],  # General condition of the basement\n",
    "    'BsmtExposure': ['None', 'No', 'Mn', 'Av', 'Gd'],  # Walkout or garden level basement walls\n",
    "    'BsmtFinType1': ['None', 'Unf', 'LwQ', 'Rec', 'BLQ', 'ALQ', 'GLQ'],  # Quality of basement finished area\n",
    "    'BsmtFinType2': ['None', 'Unf', 'LwQ', 'Rec', 'BLQ', 'ALQ', 'GLQ'],  # Quality of second basement finished area\n",
    "    'HeatingQC': ['Po', 'Fa', 'TA', 'Gd', 'Ex'],  # Heating quality and condition\n",
    "    'KitchenQual': ['Po', 'Fa', 'TA', 'Gd', 'Ex'],  # Kitchen quality\n",
    "    'Functional': ['Sal', 'Sev', 'Maj2', 'Maj1', 'Mod', 'Min2', 'Min1', 'Typ'],  # Home functionality\n",
    "    'FireplaceQu': ['None', 'Po', 'Fa', 'TA', 'Gd', 'Ex'],  # Fireplace quality\n",
    "    'GarageFinish': ['None', 'Unf', 'RFn', 'Fin'],  # Interior finish of the garage\n",
    "    'GarageQual': ['None', 'Po', 'Fa', 'TA', 'Gd', 'Ex'],  # Garage quality\n",
    "    'GarageCond': ['None', 'Po', 'Fa', 'TA', 'Gd', 'Ex'],  # Garage condition\n",
    "    'PavedDrive': ['N', 'P', 'Y'],  # Paved driveway\n",
    "    'PoolQC': ['None', 'Fa', 'TA', 'Gd', 'Ex'],  # Pool quality\n",
    "    'Fence': ['None', 'MnWw', 'GdWo', 'MnPrv', 'GdPrv']  # Fence quality\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f9a3035-0620-46e9-9c6e-586984c12b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract list of ALL ordinal features from dictionary\n",
    "ordinal_features = list(ordinal_order.keys())\n",
    "# List of ordinal features except Electrical\n",
    "ordinal_except_electrical = [feature for feature in ordinal_features if feature != 'Electrical']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa76b707-8d99-4e3d-9c14-d4578131472e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to fill 'None' for missing categorical data\n",
    "def fill_none(X):\n",
    "    return X.fillna(\"None\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2bced43-3b0e-47de-b973-b7410545e187",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline for 'Electrical': Fill missing value with mode then apply ordinal encoding\n",
    "electrical_transformer = Pipeline(steps=[\n",
    "    ('impute_electrical', SimpleImputer(strategy='most_frequent')),\n",
    "    ('ordinal_electrical', OrdinalEncoder(categories=[ordinal_order['Electrical']]))\n",
    "])\n",
    "# Pipeline for numeric features: Impute missing values using mean\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('impute_mean', SimpleImputer(strategy='mean'))\n",
    "])\n",
    "# Pipeline for ordinal features: Fill missing values with 'None' then apply ordinal encoding\n",
    "ordinal_transformer = Pipeline(steps=[\n",
    "    ('fill_none', FunctionTransformer(fill_none, validate=False)),\n",
    "    ('ordinal', OrdinalEncoder(categories=[ordinal_order[feature] for feature in ordinal_features if feature in ordinal_except_electrical]))\n",
    "])\n",
    "# Pipeline for nominal categorical features: Fill missing values with 'None' then apply one-hot encoding\n",
    "nominal_features = [feature for feature in categorical_features if feature not in ordinal_features]\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('fill_none', FunctionTransformer(fill_none, validate=False)),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "# Combined preprocessor for numeric, ordinal, nominal, and specific electrical data\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('electrical', electrical_transformer, ['Electrical']),\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('ordinal', ordinal_transformer, ordinal_except_electrical),\n",
    "        ('nominal', categorical_transformer, nominal_features)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc51f3b-ec05-48eb-ac68-f2b7a6116a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the preprocessing pipeline to Ames\n",
    "transformed_data = preprocessor.fit_transform(Ames)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3813b981-0969-4b99-9b6d-f1274ef2f5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate column names for the one-hot encoded features\n",
    "onehot_features = preprocessor.named_transformers_['nominal'].named_steps['onehot'].get_feature_names_out()\n",
    "# Combine all feature names\n",
    "all_feature_names = ['Electrical'] + list(numeric_features) + list(ordinal_except_electrical) + list(onehot_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416979ca-4482-4a7c-8050-8cd9d27b3e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the transformed array to a DataFrame\n",
    "transformed_df = pd.DataFrame(transformed_data, columns=all_feature_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf765ba-0dae-4b99-856d-1fd669a07557",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestRegressor()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921379f5-b862-49cf-8318-3b575a5d8563",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = StratifiedKFold(5)\n",
    "grid = {'n_estimators':range(30,110,30), 'max_depth':range(3,20,2)}\n",
    "gsCV = GridSearchCV(rf, grid, cv=cv, return_train_score=True, n_jobs=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b34e7b-60fd-4333-a73b-4aeeab8e8e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = gsCV.fit(transformed_df, Ames.SalePrice)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197dc373-6a65-4d51-b2eb-8566cc04521a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ans=pd.DataFrame(results.cv_results_)\n",
    "ans['mean_train_score'].plot()\n",
    "ans['mean_test_score'].plot()\n",
    "plt.xlabel('order')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.axvline(x=6,linestyle='--', color='r')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b503a535-3eb6-4a30-b2a3-1b62862e95d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ans['params'][6]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ef46af-fe63-4aed-99a7-3fce1ab99789",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf.set_params(max_depth=7, n_estimators=30)\n",
    "rf.fit(transformed_df, Ames.SalePrice)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650c2fb9-5e17-412b-9116-01b79224170d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_importance = pd.Series(rf.feature_importances_, index=transformed_df.columns).sort_values(ascending=False)\n",
    "model_importance.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543806b9-cf30-471a-a109-c32c5ca5eb17",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_importance.head(10).plot(kind=\"bar\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99c332e-311d-46f7-9d20-7523f8a8053f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ames.groupby(model_importance.index[0])['SalePrice'].mean().plot(kind=\"bar\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7888771c-b2e1-4b71-8bc8-c804c2be4259",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ames[[model_importance.index[1], 'SalePrice']].plot(kind='scatter', x=model_importance.index[1], y='SalePrice')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac134e3-5a29-416a-8ea2-6db2ec983ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ames[[model_importance.index[2], 'SalePrice']].plot(kind='scatter', x=model_importance.index[2], y='SalePrice')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5286a1a0-c758-4bf9-915d-74623077a9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaler = StandardScaler()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c88b5e5-58bd-4fe5-8335-cc3c922ba244",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = transformed_df.copy()\n",
    "# ---- 1. Define which numeric columns to log-transform ----\n",
    "log_cols = [\n",
    "    \"LotArea\", \"LotFrontage\", \"MasVnrArea\",\n",
    "    \"BsmtFinSF1\", \"BsmtFinSF2\", \"BsmtUnfSF\", \"TotalBsmtSF\",\n",
    "    \"1stFlrSF\", \"2ndFlrSF\", \"LowQualFinSF\", \"GrLivArea\",\n",
    "    \"GarageArea\", \"WoodDeckSF\", \"OpenPorchSF\", \"EnclosedPorch\",\n",
    "    \"3SsnPorch\", \"ScreenPorch\", \"PoolArea\", \"MiscVal\"\n",
    "]\n",
    "# Keep only columns that actually exist in X\n",
    "log_cols = [c for c in log_cols if c in X.columns]\n",
    "# ---- 2. Define full numeric scaling list ----\n",
    "scale_features = [\n",
    "    # Continuous\n",
    "    \"GrLivArea\", \"LotFrontage\", \"LotArea\", \"MasVnrArea\",\n",
    "    \"BsmtFinSF1\", \"BsmtFinSF2\", \"BsmtUnfSF\", \"TotalBsmtSF\",\n",
    "    \"1stFlrSF\", \"2ndFlrSF\", \"LowQualFinSF\", \"GarageArea\",\n",
    "    \"WoodDeckSF\", \"OpenPorchSF\", \"EnclosedPorch\", \"3SsnPorch\",\n",
    "    \"ScreenPorch\", \"PoolArea\", \"MiscVal\",\n",
    "# Counts\n",
    "    \"BsmtFullBath\", \"BsmtHalfBath\", \"FullBath\", \"HalfBath\",\n",
    "    \"BedroomAbvGr\", \"KitchenAbvGr\", \"TotRmsAbvGrd\",\n",
    "    \"Fireplaces\", \"GarageCars\",\n",
    "# Ordinal-as-numeric\n",
    "    \"OverallQual\", \"OverallCond\",\n",
    "# Year variables (or replace with \"age\")\n",
    "    \"YearBuilt\", \"YearRemodAdd\", \"GarageYrBlt\"\n",
    "]\n",
    "scale_features = [c for c in scale_features if c in X.columns]\n",
    "# ---- 3. Apply log transform first, then scale ----\n",
    "X_num = X[scale_features].copy()\n",
    "X_num[log_cols] = X_num[log_cols].apply(np.log1p)\n",
    "scaler = StandardScaler()\n",
    "X_scaled_num = pd.DataFrame(\n",
    "    scaler.fit_transform(X_num),\n",
    "    columns=scale_features,\n",
    "    index=X.index\n",
    ")\n",
    "# ---- 4. Combine back with one-hot columns ----\n",
    "onehot_cols = [c for c in X.columns if c not in scale_features]\n",
    "X_final = pd.concat([X_scaled_num, X[onehot_cols]], axis=1)\n",
    "target_scaled = np.log1p(Ames.SalePrice)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b067333-aeaa-4a19-81f6-e22be84b3a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descriptive statistics for numerical columns\n",
    "print(X_final.describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3bab2a2-55eb-4469-bfec-09a55b01697d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def univariate_linreg_statsmodels(\n",
    "    X: pd.DataFrame,\n",
    "    y: pd.Series,\n",
    "    *,\n",
    "    log_target: bool = False\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Runs separate OLS for each single feature vs. y.\n",
    "    If log_target=True, fits on log1p(y) and reports RMSE in dollars (back-transformed).\n",
    "    Coefficients and intercept are reported in the model's native space\n",
    "    (log-price when log_target=True; raw dollars otherwise).\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    y_array = np.asarray(y)\n",
    "    y_trainable = np.log1p(y_array) if log_target else y_array  # training target\n",
    "for col in X.columns:\n",
    "        x = X[[col]]\n",
    "        if np.allclose(x.values.std(), 0):\n",
    "            # constant feature: skip fit/metrics\n",
    "            rows.append({\n",
    "                \"feature\": col,\n",
    "                \"r2\": np.nan,\n",
    "                \"rmse\": np.nan,\n",
    "                \"coef\": 0.0,\n",
    "                \"intercept\": float(np.mean(y_trainable)),\n",
    "                \"p_value\": np.nan,\n",
    "                \"coef_ci_low\": np.nan,\n",
    "                \"coef_ci_high\": np.nan,\n",
    "                \"fitted_on_log_target\": bool(log_target)\n",
    "            })\n",
    "            continue\n",
    "Xc = sm.add_constant(x, has_constant=\"add\")\n",
    "        model = sm.OLS(y_trainable, Xc).fit()\n",
    "# Predictions in training space\n",
    "        y_pred_train_space = model.predict(Xc)\n",
    "# RMSE: report in dollars if we trained in log space\n",
    "        if log_target:\n",
    "            y_pred_dollars = np.expm1(y_pred_train_space)\n",
    "            rmse = float(np.sqrt(((y_array - y_pred_dollars) ** 2).mean()))\n",
    "        else:\n",
    "            rmse = float(np.sqrt(((y_array - y_pred_train_space) ** 2).mean()))\n",
    "coef = float(model.params[col])\n",
    "        ci_low, ci_high = model.conf_int().loc[col].tolist()\n",
    "        p = float(model.pvalues[col])\n",
    "rows.append({\n",
    "            \"feature\": col,\n",
    "            \"r2\": float(model.rsquared),\n",
    "            \"rmse\": rmse,\n",
    "            \"coef\": coef,\n",
    "            \"intercept\": float(model.params[\"const\"]),\n",
    "            \"p_value\": p,\n",
    "            \"coef_ci_low\": float(ci_low),\n",
    "            \"coef_ci_high\": float(ci_high),\n",
    "            \"fitted_on_log_target\": bool(log_target)\n",
    "        })\n",
    "out = pd.DataFrame(rows).sort_values(\"r2\", ascending=False).reset_index(drop=True)\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03569a6e-3490-49e2-b8f4-e629e27fabc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = univariate_linreg_statsmodels(X_final, Ames.SalePrice, log_target=True)\n",
    "results_df.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc5de16-629a-49cb-943a-949ec0047c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ames.groupby(results_df.feature[2])['SalePrice'].mean().plot(kind=\"bar\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89edd513-91c1-4159-8a0b-61c6cfff4977",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ames.groupby(results_df.feature[3])['SalePrice'].mean().sort_values(ascending=False).plot(kind=\"bar\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27cae83-6482-4dcf-beb5-bbfec2ea6fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Helpers ---\n",
    "def rmse_compat(y_true, y_pred):\n",
    "    mse = mean_squared_error(y_true, y_pred)  # works across sklearn versions\n",
    "    return float(np.sqrt(mse))\n",
    "def get_feature_names(X):\n",
    "    if hasattr(X, \"columns\"):\n",
    "        return list(X.columns)\n",
    "    return [f\"x{i}\" for i in range(X.shape[1])]\n",
    "# --- Main ---\n",
    "def fit_multiple_linreg_all_features(\n",
    "    X, y, *, test_size=0.2, random_state=42, n_splits=5, log_target=False\n",
    "):\n",
    "    \"\"\"\n",
    "    X: preprocessed feature matrix (DataFrame or ndarray), no NaNs\n",
    "    y: target Series/array (SalePrice)\n",
    "    log_target: if True, fits on log1p(y) and reports back-transformed RMSE too\n",
    "    \"\"\"\n",
    "    feature_names = get_feature_names(X)\n",
    "# Target transform (optional)\n",
    "    if log_target:\n",
    "        y_trainable = np.log1p(y)\n",
    "    else:\n",
    "        y_trainable = np.asarray(y)\n",
    "# Split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y_trainable, test_size=test_size, random_state=random_state\n",
    "    )\n",
    "# Fit\n",
    "    lr = LinearRegression()\n",
    "    lr.fit(X_train, y_train)\n",
    "# Predict\n",
    "    y_pred_test = lr.predict(X_test)\n",
    "    y_pred_train = lr.predict(X_train)\n",
    "# Metrics (in target space)\n",
    "    if log_target:\n",
    "        # Back-transform for error metrics in dollars\n",
    "        y_test_dollars = np.expm1(y_test)\n",
    "        y_pred_test_dollars = np.expm1(y_pred_test)\n",
    "        test_rmse = rmse_compat(y_test_dollars, y_pred_test_dollars)\n",
    "        test_r2 = r2_score(y_test_dollars, y_pred_test_dollars)\n",
    "    else:\n",
    "        test_rmse = rmse_compat(y_test, y_pred_test)\n",
    "        test_r2 = float(r2_score(y_test, y_pred_test))\n",
    "# Cross-validated RMSE (on full data)\n",
    "    cv = KFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "    neg_mse = cross_val_score(lr, X, y_trainable, scoring=\"neg_mean_squared_error\", cv=cv)\n",
    "    if log_target:\n",
    "        # Back-transform each fold's MSE is not available; report in log space as approx\n",
    "        cv_rmse = float(np.sqrt(-neg_mse).mean())\n",
    "        cv_rmse_std = float(np.sqrt(-neg_mse).std())\n",
    "    else:\n",
    "        cv_rmse = float(np.sqrt(-neg_mse).mean())\n",
    "        cv_rmse_std = float(np.sqrt(-neg_mse).std())\n",
    "# Coefficients\n",
    "    coef_df = pd.DataFrame({\n",
    "        \"feature\": feature_names,\n",
    "        \"coef\": lr.coef_,\n",
    "        \"abs_coef\": np.abs(lr.coef_)\n",
    "    }).sort_values(\"abs_coef\", ascending=False).reset_index(drop=True)\n",
    "metrics = {\n",
    "        \"test_rmse\": test_rmse,\n",
    "        \"test_r2\": test_r2,\n",
    "        \"cv_rmse_mean\": cv_rmse,\n",
    "        \"cv_rmse_std\": cv_rmse_std,\n",
    "        \"intercept\": float(lr.intercept_),\n",
    "        \"fitted_on_log_target\": bool(log_target),\n",
    "    }\n",
    "# Also return out-of-sample predictions (useful for residual checks)\n",
    "    outputs = {\n",
    "        \"y_test_pred\": y_pred_test if not log_target else y_pred_test_dollars,\n",
    "        \"y_test_true\": y_test if not log_target else y_test_dollars,\n",
    "    }\n",
    "return lr, coef_df, metrics, outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd7e98f-35e9-4c6c-ac95-27b10a3e968d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr, coef_df, metrics, out = fit_multiple_linreg_all_features(X_final, Ames[\"SalePrice\"], log_target=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779430fe-7eec-4204-ba4f-f196b506e4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad566d9-bf62-4616-8b6a-d7ff693d4612",
   "metadata": {},
   "outputs": [],
   "source": [
    "coef_df.head(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56292afa-ad5c-483c-a14f-bba2b2f037e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals = pd.Series(out[\"y_test_true\"] - out[\"y_test_pred\"], name=\"residuals\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16baa294-8ece-4160-8bb3-a485d2bface3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def coef_to_pct_change(coef_df):\n",
    "    df_pct = coef_df.copy()\n",
    "    df_pct[\"pct_change\"] = (np.exp(df_pct[\"coef\"]) - 1) * 100\n",
    "    return df_pct[[\"feature\", \"coef\", \"pct_change\"]].sort_values(\n",
    "        \"pct_change\", key=abs, ascending=False\n",
    "    ).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b67dbb-8212-4f58-bcc2-c5be583a7a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "coef_pct_df = coef_to_pct_change(coef_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65ce033-fe41-459d-9950-382b59b38b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "# coef_pct_df must already have columns: [\"feature\", \"coef\", \"pct_change\"]\n",
    "# X_final is your design matrix used to fit the model\n",
    "def is_one_hot(col: pd.Series) -> bool:\n",
    "    vals = pd.unique(col.dropna())\n",
    "    # allow 0/1 in int or float form\n",
    "    return set(np.asarray(vals)).issubset({0, 1, 0.0, 1.0})\n",
    "# 1) Annotate feature type + counts/coverage\n",
    "feature_types = {}\n",
    "counts = {}\n",
    "pct_of_data = {}\n",
    "n = len(X_final)\n",
    "for feat in coef_pct_df[\"feature\"]:\n",
    "    if feat in X_final.columns:\n",
    "        if is_one_hot(X_final[feat]):\n",
    "            feature_types[feat] = \"one-hot categorical\"\n",
    "            counts[feat] = int(X_final[feat].sum())\n",
    "            pct_of_data[feat] = (counts[feat] / n) * 100.0\n",
    "        else:\n",
    "            feature_types[feat] = \"numeric\"\n",
    "            counts[feat] = None\n",
    "            pct_of_data[feat] = 100.0  # numeric features apply to all rows\n",
    "    else:\n",
    "        feature_types[feat] = \"unknown\"\n",
    "        counts[feat] = None\n",
    "        pct_of_data[feat] = np.nan\n",
    "coef_pct_df[\"feature_type\"] = coef_pct_df[\"feature\"].map(feature_types)\n",
    "coef_pct_df[\"count\"] = coef_pct_df[\"feature\"].map(counts)\n",
    "coef_pct_df[\"pct_of_data\"] = coef_pct_df[\"feature\"].map(pct_of_data)\n",
    "def rarity_label(row):\n",
    "    if row[\"feature_type\"] == \"numeric\":\n",
    "        return \"Numeric (N/A)\"\n",
    "    if pd.isna(row[\"pct_of_data\"]):\n",
    "        return \"N/A\"\n",
    "    if row[\"pct_of_data\"] < 1:\n",
    "        return \"Very rare (<1%)\"\n",
    "    if row[\"pct_of_data\"] < 5:\n",
    "        return \"Rare (1–5%)\"\n",
    "    return \"Common (≥5%)\"\n",
    "coef_pct_df[\"rarity\"] = coef_pct_df.apply(rarity_label, axis=1)\n",
    "# Sort by absolute % change\n",
    "coef_pct_df = coef_pct_df.sort_values(\"pct_change\", key=np.abs, ascending=False).reset_index(drop=True)\n",
    "# 2) Filter to \"common\": numeric OR ≥ 5% coverage\n",
    "common_mask = (coef_pct_df[\"feature_type\"] == \"numeric\") | (coef_pct_df[\"pct_of_data\"] >= 5)\n",
    "common_coef_df = (\n",
    "    coef_pct_df.loc[common_mask, [\"feature\", \"feature_type\", \"pct_change\", \"count\", \"pct_of_data\", \"rarity\"]]\n",
    "    .sort_values(\"pct_change\", key=np.abs, ascending=False)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "print(\"=== FULL LIST WITH RARITY ===\")\n",
    "display(coef_pct_df[[\"feature\", \"feature_type\", \"pct_change\", \"count\", \"pct_of_data\", \"rarity\"]])\n",
    "print(\"\\n=== COMMON FEATURES (numeric OR ≥5% one-hot) ===\")\n",
    "display(common_coef_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a90a27-0b65-495a-8480-b799752b6e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "common_coef_df['pct_change_abs'] = abs(common_coef_df['pct_change'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df133d2b-6400-43de-aea7-31fde55c9a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "common_coef_df.sort_values('pct_change_abs', ascending = False).head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d49ee99-8964-490f-93fb-f1ad00d4d44d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge = Ridge()\n",
    "lasso = Lasso()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444b7aed-1a20-4f46-bebe-ef94fe64868d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge.fit(X_final, np.log1p(Ames.SalePrice))\n",
    "print('the ridge intercept is: %.2f' %(ridge.intercept_))\n",
    "pd.Series(ridge.coef_, index=X_final.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b12f26a-74bc-4808-8699-582e44c845f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge.score(X_final,np.log1p(Ames.SalePrice))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863824c0-2e44-4b93-aac7-5bf2d30dcbab",
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso.fit(X_final, np.log1p(Ames.SalePrice))\n",
    "print('the lasso intercept is: %.2f' %(lasso.intercept_))\n",
    "pd.Series(lasso.coef_, index=X_final.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9bf4453-b89b-4245-9858-fc107dfe475b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso.score(X_final, np.log1p(Ames.SalePrice))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33eb89ec-8b20-472f-8f04-37acbe4f581e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X_final,np.log1p(Ames.SalePrice),test_size=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b25070a-8dd6-4cb0-93b7-a5b6a5e3ee12",
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_scores_train = []\n",
    "lasso_scores_train = []\n",
    "ridge_scores_test  = []\n",
    "lasso_scores_test  = []\n",
    "alphas = np.linspace(0.01, 1, 100)\n",
    "for alpha in alphas:\n",
    "            ridge.set_params(alpha=alpha)\n",
    "            lasso.set_params(alpha=alpha)\n",
    "            ridge.fit(X_train, Y_train)\n",
    "            lasso.fit(X_train, Y_train)\n",
    "            ridge_scores_train.append(ridge.score(X_train, Y_train))\n",
    "            ridge_scores_test.append(ridge.score(X_test, Y_test))\n",
    "            lasso_scores_train.append(lasso.score(X_train, Y_train))\n",
    "            lasso_scores_test.append(lasso.score(X_test, Y_test))\n",
    "ridge_scores_train = np.array(ridge_scores_train)\n",
    "ridge_scores_test  = np.array(ridge_scores_test)\n",
    "lasso_scores_train = np.array(lasso_scores_train)\n",
    "lasso_scores_test  = np.array(lasso_scores_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a2edef-fcbb-42e7-b0ac-aa4d2be41e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(alphas, ridge_scores_train, label=r'$train\\ R^2$')\n",
    "plt.plot(alphas, ridge_scores_test, label=r'$test\\ R^2$')\n",
    "plt.legend(loc=1)\n",
    "plt.title(r'Ridge Train-Test $R^2$ Comparison')\n",
    "ridge_underfit = ridge_scores_train < ridge_scores_test\n",
    "# last_underfit  = np.max(alphas[ridge_underfit])\n",
    "# plt.axvline(last_underfit, linestyle='--', color='g', label='optimal lambda', alpha=0.4)\n",
    "plt.legend(loc=1)\n",
    "plt.xlabel(r'hyperparameter $\\lambda$')\n",
    "plt.ylabel(r'$R^2$')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f20de75-da95-421c-be8d-a8d809ceff5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RidgeCV, LassoCV, ElasticNetCV\n",
    "from sklearn.model_selection import KFold\n",
    "# Example: ridge with CV over alphas\n",
    "alphas = [0.01, 0.1, 1, 10, 50, 100, 200]\n",
    "ridge = RidgeCV(alphas=alphas, cv=KFold(10, shuffle=True, random_state=42), scoring=\"neg_root_mean_squared_error\")\n",
    "ridge.fit(X_final, np.log1p(Ames[\"SalePrice\"]))\n",
    "print(\"Best alpha:\", ridge.alpha_)\n",
    "print(\"RMSE (log target):\", -ridge.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4bee579-72fa-4a87-821e-be3011e7e89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_penalized_models_stable(\n",
    "    X, y, *,\n",
    "    log_target=True,\n",
    "    random_state=42,\n",
    "    top_n=50,            # collect up to top_n by |coef| before filtering/sorting\n",
    "    min_pct=5.0,         # keep one-hot features seen in ≥ min_pct of homes\n",
    "    include_full=False   # also return unfiltered coef tables\n",
    "):\n",
    "    \"\"\"\n",
    "    Fits Ridge, Lasso, ElasticNet with CV, returns metrics and filtered coef tables\n",
    "    with % changes, feature types, and coverage. Keeps all numeric features and\n",
    "    one-hot features present in ≥ min_pct of homes.\n",
    "    \"\"\"\n",
    "    y_array = np.asarray(y)\n",
    "    y_trainable = np.log1p(y_array) if log_target else y_array\n",
    "# --- CV and search grids ---\n",
    "    cv = KFold(n_splits=10, shuffle=True, random_state=random_state)\n",
    "    alphas = np.logspace(-3, 3, 50)\n",
    "    l1_ratios = [0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "# --- Fit models ---\n",
    "    ridge = RidgeCV(alphas=alphas, cv=cv, scoring=\"neg_mean_squared_error\").fit(X, y_trainable)\n",
    "    lasso = LassoCV(alphas=alphas, cv=cv, random_state=random_state, max_iter=5000).fit(X, y_trainable)\n",
    "    enet  = ElasticNetCV(alphas=alphas, l1_ratio=l1_ratios, cv=cv,\n",
    "                         random_state=random_state, max_iter=5000).fit(X, y_trainable)\n",
    "# --- Metrics helper ---\n",
    "    def _metrics(name, model, y_true, y_pred_train_space):\n",
    "        if log_target:\n",
    "            y_pred = np.expm1(y_pred_train_space)\n",
    "            rmse = float(np.sqrt(mean_squared_error(y_true, y_pred)))\n",
    "            r2   = float(r2_score(y_true, y_pred))\n",
    "        else:\n",
    "            rmse = float(np.sqrt(mean_squared_error(y_true, y_pred_train_space)))\n",
    "            r2   = float(r2_score(y_true, y_pred_train_space))\n",
    "        return {\n",
    "            \"model\": name,\n",
    "            \"alpha\": getattr(model, \"alpha_\", None),\n",
    "            \"l1_ratio\": getattr(model, \"l1_ratio_\", None),\n",
    "            \"rmse\": rmse,\n",
    "            \"r2\": r2\n",
    "        }\n",
    "results = pd.DataFrame([\n",
    "        _metrics(\"Ridge\",      ridge, y_array, ridge.predict(X)),\n",
    "        _metrics(\"Lasso\",      lasso, y_array, lasso.predict(X)),\n",
    "        _metrics(\"ElasticNet\", enet,  y_array, enet.predict(X)),\n",
    "    ])\n",
    "# --- Build coef tables ---\n",
    "    def _coef_table(model, name):\n",
    "        if hasattr(X, \"columns\"):\n",
    "            feats = list(X.columns)\n",
    "        else:\n",
    "            feats = [f\"x{i}\" for i in range(X.shape[1])]\n",
    "        coef = np.asarray(model.coef_)\n",
    "        df = pd.DataFrame({\"feature\": feats, \"coef\": coef})\n",
    "        df[\"abs_coef\"] = df[\"coef\"].abs()\n",
    "        df = df.sort_values(\"abs_coef\", ascending=False).head(top_n).reset_index(drop=True)\n",
    "        return df\n",
    "raw_tables = {\n",
    "        \"Ridge\": _coef_table(ridge, \"Ridge\"),\n",
    "        \"Lasso\": _coef_table(lasso, \"Lasso\"),\n",
    "        \"ElasticNet\": _coef_table(enet, \"ElasticNet\"),\n",
    "    }\n",
    "# --- Add % change, feature type, and coverage; then filter ---\n",
    "    def _is_one_hot(col: pd.Series) -> bool:\n",
    "        vals = pd.unique(col.dropna())\n",
    "        return set(np.asarray(vals)).issubset({0, 1, 0.0, 1.0})\n",
    "n = len(X) if hasattr(X, \"__len__\") else np.nan\n",
    "    if hasattr(X, \"columns\"):\n",
    "        # detect once\n",
    "        is_dummy = {c: _is_one_hot(X[c]) for c in X.columns}\n",
    "        coverage = {c: (float(X[c].sum()) / n * 100.0) if is_dummy[c] else 100.0 for c in X.columns}\n",
    "    else:\n",
    "        is_dummy, coverage = {}, {}\n",
    "def _augment_and_filter(df):\n",
    "        out = df.copy()\n",
    "        # % change (only meaningful if log_target=True)\n",
    "        out[\"pct_change\"] = (np.exp(out[\"coef\"]) - 1.0) * 100.0 if log_target else np.nan\n",
    "        # feature type & coverage\n",
    "        if hasattr(X, \"columns\"):\n",
    "            out[\"feature_type\"] = out[\"feature\"].map(lambda f: \"one-hot categorical\" if is_dummy.get(f, False) else \"numeric\")\n",
    "            out[\"pct_of_data\"]  = out[\"feature\"].map(lambda f: coverage.get(f, np.nan))\n",
    "        else:\n",
    "            out[\"feature_type\"] = \"unknown\"\n",
    "            out[\"pct_of_data\"] = np.nan\n",
    "        # filter: keep all numeric OR one-hot with ≥ min_pct coverage\n",
    "        mask = (out[\"feature_type\"] == \"numeric\") | (out[\"pct_of_data\"] >= min_pct)\n",
    "        out = out[mask].copy()\n",
    "        # sort by abs % change (fallback to abs coef if not logged)\n",
    "        if log_target:\n",
    "            out = out.reindex(out[\"pct_change\"].abs().sort_values(ascending=False).index)\n",
    "        else:\n",
    "            out = out.sort_values(\"abs_coef\", ascending=False)\n",
    "        out.reset_index(drop=True, inplace=True)\n",
    "        return out\n",
    "filtered_tables = {name: _augment_and_filter(df) for name, df in raw_tables.items()}\n",
    "return results, filtered_tables if not include_full else (results, filtered_tables, raw_tables)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce49195c-18e5-42b6-8acd-f60e2b792c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# log target recommended for Ames\n",
    "results_df, coef_tables = evaluate_penalized_models_stable(\n",
    "    X_final, Ames[\"SalePrice\"], log_target=True, min_pct=5, top_n=200\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c05c5ae-d07d-4594-87d4-628a3c5c59f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results_df)\n",
    "print(\"\\nRidge (common):\\n\", coef_tables[\"Ridge\"][[\"feature\",\"feature_type\",\"pct_change\",\"pct_of_data\"]].head(15))\n",
    "print(\"\\nLasso (common):\\n\", coef_tables[\"Lasso\"][[\"feature\",\"feature_type\",\"pct_change\",\"pct_of_data\"]].head(15))\n",
    "print(\"\\nElasticNet (common):\\n\", coef_tables[\"ElasticNet\"][[\"feature\",\"feature_type\",\"pct_change\",\"pct_of_data\"]].head(15))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316f45bd-aa16-4101-9bce-55c8c103f148",
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = np.logspace(-3, 1, 40)  # widen if needed\n",
    "l1_grid = [0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "models = [\n",
    "    (\"Ridge\", Ridge(alpha=14.56)),  # keep fixed if you like\n",
    "    (\"LassoCV\", LassoCV(alphas=alphas, cv=5, max_iter=100_000, tol=1e-3, n_jobs=-1)),\n",
    "    (\"ElasticNetCV\", ElasticNetCV(alphas=alphas, l1_ratio=l1_grid, cv=5, max_iter=100_000, tol=1e-3, n_jobs=-1)),\n",
    "    (\"RandomForest\", RandomForestRegressor(n_estimators=500, random_state=42, n_jobs=-1)),\n",
    "    (\"HistGB\", HistGradientBoostingRegressor(random_state=42)),\n",
    "    (\"SVR\", SVR(kernel='rbf', C=10, epsilon=0.2)),\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa2ec10-64d9-4569-b45a-2681a3075f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "# ---------- 1) Build consensus table ----------\n",
    "def build_consensus_table(ridge_df: pd.DataFrame,\n",
    "                          lasso_df: pd.DataFrame,\n",
    "                          enet_df: pd.DataFrame,\n",
    "                          min_models: int = 2) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Merge Ridge/Lasso/ElasticNet common-tables, compute average % change and std.\n",
    "    Keeps features that appear in at least `min_models` of the three tables.\n",
    "    \"\"\"\n",
    "    # Keep only needed cols and rename pct_change to model-specific names\n",
    "    def _prep(df, model_name):\n",
    "        return (df[[\"feature\", \"feature_type\", \"pct_change\", \"pct_of_data\"]]\n",
    "                  .rename(columns={\"pct_change\": f\"{model_name}_pct\",\n",
    "                                   \"pct_of_data\": f\"{model_name}_pct_of_data\"}))\n",
    "r = _prep(ridge_df.copy(), \"Ridge\")\n",
    "    l = _prep(lasso_df.copy(), \"Lasso\")\n",
    "    e = _prep(enet_df.copy(),  \"ElasticNet\")\n",
    "# Outer-join on feature + type\n",
    "    merged = r.merge(l, on=[\"feature\", \"feature_type\"], how=\"outer\") \\\n",
    "              .merge(e, on=[\"feature\", \"feature_type\"], how=\"outer\")\n",
    "# Count how many models have a pct value\n",
    "    model_cols = [\"Ridge_pct\", \"Lasso_pct\", \"ElasticNet_pct\"]\n",
    "    merged[\"n_models\"] = merged[model_cols].notna().sum(axis=1)\n",
    "# Keep features present in at least `min_models`\n",
    "    merged = merged[merged[\"n_models\"] >= min_models].copy()\n",
    "# Avg / std across available models\n",
    "    merged[\"avg_pct_change\"] = merged[model_cols].mean(axis=1, skipna=True)\n",
    "    merged[\"std_pct_change\"] = merged[model_cols].std(axis=1, ddof=0, skipna=True)\n",
    "# Optional: a single coverage column (max across models; they should be similar)\n",
    "    coverage_cols = [\"Ridge_pct_of_data\", \"Lasso_pct_of_data\", \"ElasticNet_pct_of_data\"]\n",
    "    if any(c in merged.columns for c in coverage_cols):\n",
    "        merged[\"pct_of_data\"] = merged[coverage_cols].max(axis=1, skipna=True)\n",
    "# Order by absolute average effect\n",
    "    merged = (merged\n",
    "              .sort_values(\"avg_pct_change\", key=lambda s: s.abs(), ascending=False)\n",
    "              .reset_index(drop=True))\n",
    "return merged\n",
    "# ---------- 2) Plot consensus bar chart ----------\n",
    "def plot_consensus_bar(consensus_df: pd.DataFrame,\n",
    "                       top_n: int = 15,\n",
    "                       with_errorbars: bool = True,\n",
    "                       figsize=(9, 6),\n",
    "                       title=\"Consensus Feature Importance (Penalized Models)\"):\n",
    "    \"\"\"\n",
    "    Horizontal bar chart of average % change (per 1 SD for numeric, vs baseline for dummies).\n",
    "    \"\"\"\n",
    "    df = consensus_df.head(top_n).copy()\n",
    "    df = df.sort_values(\"avg_pct_change\", ascending=True)\n",
    "plt.figure(figsize=figsize)\n",
    "    bars = plt.barh(df[\"feature\"], df[\"avg_pct_change\"])\n",
    "# Color by sign (optional; remove if you prefer default)\n",
    "    for b, val in zip(bars, df[\"avg_pct_change\"]):\n",
    "        b.set_color(\"tab:red\" if val < 0 else \"tab:green\")\n",
    "if with_errorbars and \"std_pct_change\" in df.columns:\n",
    "        # Draw error bars manually (centered at bar ends)\n",
    "        x = df[\"avg_pct_change\"].values\n",
    "        err = df[\"std_pct_change\"].fillna(0).values\n",
    "        y_positions = np.arange(len(df))\n",
    "        plt.errorbar(x, y_positions, xerr=err, fmt=\"none\", ecolor=\"gray\", capsize=3, linewidth=1)\n",
    "plt.axvline(0, linewidth=0.8, color=\"black\")\n",
    "    plt.xlabel(\"Average % Change in Price (per 1 SD or category)\")\n",
    "    plt.ylabel(\"Feature\")\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082dc1df-3939-4f24-9674-6f3b6b060c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_df = coef_tables[\"Ridge\"]\n",
    "lasso_df = coef_tables[\"Lasso\"]\n",
    "enet_df  = coef_tables[\"ElasticNet\"]\n",
    "# Build consensus\n",
    "consensus = build_consensus_table(ridge_df, lasso_df, enet_df, min_models=2)\n",
    "# See the table\n",
    "display(consensus[[\"feature\", \"feature_type\", \"avg_pct_change\", \"std_pct_change\", \"pct_of_data\"]].head(20))\n",
    "# Plot\n",
    "plot_consensus_bar(consensus, top_n=15, with_errorbars=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3b8202-a2b1-40dd-bd7a-d5bf9b6b8fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_final = Ames.SalePrice\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b366fac-908c-48ce-bdbb-7ed640272774",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import numpy as np\n",
    "rf = RandomForestRegressor(n_estimators=500, random_state=42, n_jobs=-1)\n",
    "scores = cross_val_score(rf, X_final, y_final, scoring=\"neg_root_mean_squared_error\", cv=5)\n",
    "print(f\"CV RMSE: {-scores.mean():.2f} ± {scores.std():.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da021fea-7c8f-4710-aa0c-544f51a59592",
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = np.logspace(-3, 1, 40)  # widen if needed\n",
    "l1_grid = [0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "models = [\n",
    "    (\"Ridge\", Ridge(alpha=14.56)),  # keep fixed if you like\n",
    "    (\"LassoCV\", LassoCV(alphas=alphas, cv=5, max_iter=100_000, tol=1e-3, n_jobs=-1)),\n",
    "    (\"ElasticNetCV\", ElasticNetCV(alphas=alphas, l1_ratio=l1_grid, cv=5, max_iter=100_000, tol=1e-3, n_jobs=-1)),\n",
    "    (\"RandomForest\", RandomForestRegressor(n_estimators=500, random_state=42, n_jobs=-1)),\n",
    "    (\"HistGB\", HistGradientBoostingRegressor(random_state=42)),\n",
    "    (\"SVR\", SVR(kernel='rbf', C=10, epsilon=0.2)),\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405ec989-1367-4d0e-bc61-926e307d0fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "def rmse(y_true, y_pred):\n",
    "    return float(np.sqrt(mean_squared_error(y_true, y_pred)))\n",
    "def cv_eval_models(X, y, models, *, cv=5, log_target=False, random_state=42):\n",
    "    \"\"\"\n",
    "    Cross-validated evaluation with optional log-target training and dollar-scale metrics.\n",
    "Returns a DataFrame with:\n",
    "      - cv_rmse_mean / cv_rmse_std  (in $)\n",
    "      - r2_cv   (R² on out-of-fold predictions, in $ space)\n",
    "      - r2_full (R² when fitting the model on all data, in $ space)\n",
    "    \"\"\"\n",
    "    # Ensure numpy views for y when needed\n",
    "    if hasattr(y, \"to_numpy\"):\n",
    "        y_np = y.to_numpy()\n",
    "    else:\n",
    "        y_np = np.asarray(y)\n",
    "# For X, keep as-is (can be DataFrame or ndarray); we'll iloc if available\n",
    "    kf = KFold(n_splits=cv, shuffle=True, random_state=random_state)\n",
    "results = []\n",
    "    for name, model in models:\n",
    "        fold_rmses = []\n",
    "        oof_pred = np.zeros(len(y_np), dtype=float)\n",
    "for train_idx, val_idx in kf.split(X):\n",
    "            # X positional split\n",
    "            if hasattr(X, \"iloc\"):\n",
    "                X_tr, X_va = X.iloc[train_idx], X.iloc[val_idx]\n",
    "            else:\n",
    "                X_tr, X_va = X[train_idx], X[val_idx]\n",
    "# y positional split (always numpy)\n",
    "            y_tr, y_va = y_np[train_idx], y_np[val_idx]\n",
    "# Train on raw or log target\n",
    "            y_tr_used = np.log1p(y_tr) if log_target else y_tr\n",
    "            model.fit(X_tr, y_tr_used)\n",
    "# Predict and back-transform if needed\n",
    "            y_va_pred = model.predict(X_va)\n",
    "            if log_target:\n",
    "                y_va_pred = np.expm1(y_va_pred)\n",
    "# Fold metrics (dollar space)\n",
    "            fold_rmses.append(rmse(y_va, y_va_pred))\n",
    "            oof_pred[val_idx] = y_va_pred\n",
    "# Overall CV metrics (OOF predictions in dollar space)\n",
    "        r2_cv = r2_score(y_np, oof_pred)\n",
    "# Fit on full data for full-data R² (still in dollar space)\n",
    "        y_full_used = np.log1p(y_np) if log_target else y_np\n",
    "        if hasattr(X, \"iloc\"):\n",
    "            model.fit(X, y_full_used)\n",
    "            y_full_pred = model.predict(X)\n",
    "        else:\n",
    "            model.fit(X, y_full_used)\n",
    "            y_full_pred = model.predict(X)\n",
    "        if log_target:\n",
    "            y_full_pred = np.expm1(y_full_pred)\n",
    "        r2_full = r2_score(y_np, y_full_pred)\n",
    "results.append({\n",
    "            \"model\": name,\n",
    "            \"cv_rmse_mean\": float(np.mean(fold_rmses)),\n",
    "            \"cv_rmse_std\":  float(np.std(fold_rmses, ddof=0)),\n",
    "            \"r2_cv\":  float(r2_cv),\n",
    "            \"r2_full\": float(r2_full),\n",
    "        })\n",
    "return pd.DataFrame(results).sort_values(\"cv_rmse_mean\").reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0d1320-acf9-4e98-ac75-0a9cdb96dff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAW target\n",
    "results_raw = cv_eval_models(X_final, y_final, models, cv=5, log_target=False, random_state=42)\n",
    "print(\"\\n=== RAW target leaderboard ===\")\n",
    "print(results_raw.to_string(index=False))\n",
    "# LOG target (RMSE back-transformed to dollars)\n",
    "results_log = cv_eval_models(X_final, y_final, models, cv=5, log_target=True, random_state=42)\n",
    "print(\"\\n=== LOG target leaderboard ===\")\n",
    "print(results_log.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eed517a-c5df-46cf-913d-e26fc27ed319",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet\n",
    "from sklearn.ensemble import RandomForestRegressor, HistGradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import cross_val_predict, KFold\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "# ---------- helpers ----------\n",
    "def rmse(y_true, y_pred):\n",
    "    # version-proof RMSE (no squared=False)\n",
    "    return float(np.sqrt(mean_squared_error(y_true, y_pred)))\n",
    "from joblib import parallel_backend\n",
    "def evaluate_model(model, X, y, *, log_target=False, cv=5, random_state=42, n_jobs_cv=1, backend=\"threading\"):\n",
    "    y_np = y.to_numpy() if hasattr(y, \"to_numpy\") else np.asarray(y)\n",
    "    splitter = KFold(n_splits=cv, shuffle=True, random_state=random_state)\n",
    "    y_used = np.log1p(y_np) if log_target else y_np\n",
    "# threads avoid the multiprocessing resource_tracker\n",
    "    with parallel_backend(backend):\n",
    "        y_pred = cross_val_predict(model, X, y_used, cv=splitter, n_jobs=n_jobs_cv)\n",
    "if log_target:\n",
    "        y_pred = np.expm1(y_pred)\n",
    "    return rmse(y_np, y_pred), r2_score(y_np, y_pred)\n",
    "# ---------- models ----------\n",
    "# models = [\n",
    "#     (\"Ridge\", Ridge(alpha=14.56)),\n",
    "#     (\"Lasso\", Lasso(alpha=0.001)),\n",
    "#     (\"ElasticNet\", ElasticNet(alpha=0.0013, l1_ratio=0.1)),\n",
    "#     (\"RandomForest\", RandomForestRegressor(n_estimators=500, random_state=42, n_jobs=-1)),\n",
    "#     (\"HistGB\", HistGradientBoostingRegressor(random_state=42)),\n",
    "#     (\"SVR\", SVR(kernel='rbf', C=10, epsilon=0.2))\n",
    "# ]\n",
    "# ---------- evaluate (raw + log) ----------\n",
    "results = []\n",
    "for log_opt in [False, True]:\n",
    "    target_name = \"log\" if log_opt else \"raw\"\n",
    "    for name, model in models:\n",
    "        rmse_val, r2_val = evaluate_model(model, X_final, y_final, log_target=log_opt, cv=5, random_state=42)\n",
    "        results.append({\"target\": target_name, \"model\": name, \"cv_rmse_mean\": rmse_val, \"r2\": r2_val})\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df.sort_values([\"target\", \"cv_rmse_mean\"]).to_string(index=False))\n",
    "# ---------- prepare pivots ----------\n",
    "rmse_pivot = results_df.pivot(index=\"model\", columns=\"target\", values=\"cv_rmse_mean\")\n",
    "r2_pivot   = results_df.pivot(index=\"model\", columns=\"target\", values=\"r2\")\n",
    "# Ensure both columns exist even if something failed earlier\n",
    "for col in [\"raw\", \"log\"]:\n",
    "    if col not in rmse_pivot.columns:\n",
    "        rmse_pivot[col] = np.nan\n",
    "    if col not in r2_pivot.columns:\n",
    "        r2_pivot[col] = np.nan\n",
    "# ---------- combined figure ----------\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "ax_rmse, ax_r2 = axes\n",
    "# ----- Left: RMSE bars -----\n",
    "x = np.arange(len(rmse_pivot.index))\n",
    "width = 0.35\n",
    "ax_rmse.bar(x - width/2, rmse_pivot[\"raw\"].values, width, label=\"Raw\")\n",
    "ax_rmse.bar(x + width/2, rmse_pivot[\"log\"].values, width, label=\"Log\")\n",
    "ax_rmse.set_ylabel(\"Cross-Validated RMSE ($)\")\n",
    "ax_rmse.set_title(\"RMSE: Raw vs. Log Target\")\n",
    "ax_rmse.set_xticks(x)\n",
    "ax_rmse.set_xticklabels(rmse_pivot.index, rotation=45, ha=\"right\")\n",
    "ax_rmse.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "ax_rmse.legend()\n",
    "# Improvement labels above taller bar\n",
    "for i, model in enumerate(rmse_pivot.index):\n",
    "    raw_val = rmse_pivot.loc[model, \"raw\"]\n",
    "    log_val = rmse_pivot.loc[model, \"log\"]\n",
    "    if np.isfinite(raw_val) and np.isfinite(log_val):\n",
    "        improvement = (raw_val - log_val) / raw_val * 100.0\n",
    "        y_top = max(raw_val, log_val)\n",
    "        ax_rmse.text(i, y_top * 1.02, f\"{improvement:+.1f}%\", ha=\"center\", va=\"bottom\", fontsize=9, fontweight=\"bold\")\n",
    "# ----- Right: R² bars -----\n",
    "ax_r2.bar(x - width/2, r2_pivot[\"raw\"].values, width, label=\"Raw\")\n",
    "ax_r2.bar(x + width/2, r2_pivot[\"log\"].values, width, label=\"Log\")\n",
    "ax_r2.set_ylabel(\"Cross-Validated R²\")\n",
    "ax_r2.set_title(\"R²: Raw vs. Log Target\")\n",
    "ax_r2.set_xticks(x)\n",
    "ax_r2.set_xticklabels(r2_pivot.index, rotation=45, ha=\"right\")\n",
    "ax_r2.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "ax_r2.legend()\n",
    "# ΔR² labels above taller bar\n",
    "for i, model in enumerate(r2_pivot.index):\n",
    "    r2_raw = r2_pivot.loc[model, \"raw\"]\n",
    "    r2_log = r2_pivot.loc[model, \"log\"]\n",
    "    if np.isfinite(r2_raw) and np.isfinite(r2_log):\n",
    "        delta = r2_log - r2_raw\n",
    "        y_top = max(r2_raw, r2_log)\n",
    "        ax_r2.text(i, y_top * 1.02, f\"{delta:+.3f} ΔR²\", ha=\"center\", va=\"bottom\", fontsize=9, fontweight=\"bold\")\n",
    "plt.suptitle(\"Model Performance Comparison (Ames)\", y=1.02, fontsize=13)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7235691c-9136-4263-a1b0-ee9f9b94d3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.ensemble import RandomForestRegressor, HistGradientBoostingRegressor, ExtraTreesRegressor, GradientBoostingRegressor\n",
    "# ---------- helpers ----------\n",
    "def rmse(y_true, y_pred):\n",
    "    return float(np.sqrt(mean_squared_error(y_true, y_pred)))\n",
    "def cv_eval_models_nonlinear(\n",
    "    X_raw, y, *,\n",
    "    cv=5,\n",
    "    log_target=False,\n",
    "    random_state=42,\n",
    "    models=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Cross-validated evaluation for NON-LINEAR (tree-based) models on RAW (unscaled) features.\n",
    "    Works with one-hot encoded categoricals; do NOT standardize numeric features.\n",
    "Returns a DataFrame with:\n",
    "      - cv_rmse_mean / cv_rmse_std  (in $)\n",
    "      - r2_cv   (R² on out-of-fold predictions, in $ space)\n",
    "      - r2_full (R² when fitting on all data, in $ space)\n",
    "    \"\"\"\n",
    "    # Default model set (all non-linear, no scaling needed)\n",
    "    if models is None:\n",
    "        models = [\n",
    "            (\"HistGB\",       HistGradientBoostingRegressor(random_state=random_state)),\n",
    "            (\"RandomForest\", RandomForestRegressor(n_estimators=800, random_state=random_state, n_jobs=-1)),\n",
    "            (\"ExtraTrees\",   ExtraTreesRegressor(n_estimators=800, random_state=random_state, n_jobs=-1)),\n",
    "            (\"GradBoost\",    GradientBoostingRegressor(random_state=random_state)),\n",
    "        ]\n",
    "# y as numpy for robust positional indexing\n",
    "    y_np = y.to_numpy() if hasattr(y, \"to_numpy\") else np.asarray(y)\n",
    "kf = KFold(n_splits=cv, shuffle=True, random_state=random_state)\n",
    "    results = []\n",
    "for name, model in models:\n",
    "        fold_rmses = []\n",
    "        oof_pred = np.zeros(len(y_np), dtype=float)\n",
    "for tr_idx, va_idx in kf.split(X_raw):\n",
    "            # positional split for X\n",
    "            if hasattr(X_raw, \"iloc\"):\n",
    "                X_tr, X_va = X_raw.iloc[tr_idx], X_raw.iloc[va_idx]\n",
    "            else:\n",
    "                X_tr, X_va = X_raw[tr_idx], X_raw[va_idx]\n",
    "# positional split for y\n",
    "            y_tr, y_va = y_np[tr_idx], y_np[va_idx]\n",
    "# train on raw or log target\n",
    "            y_tr_used = np.log1p(y_tr) if log_target else y_tr\n",
    "            model.fit(X_tr, y_tr_used)\n",
    "# predict; back-transform if needed\n",
    "            y_va_pred = model.predict(X_va)\n",
    "            if log_target:\n",
    "                y_va_pred = np.expm1(y_va_pred)\n",
    "# accumulate fold metrics in $ space\n",
    "            fold_rmses.append(rmse(y_va, y_va_pred))\n",
    "            oof_pred[va_idx] = y_va_pred\n",
    "# CV metrics from OOF preds (dollar space)\n",
    "        r2_cv = r2_score(y_np, oof_pred)\n",
    "# Full-data fit (dollar space)\n",
    "        y_full_used = np.log1p(y_np) if log_target else y_np\n",
    "        model.fit(X_raw, y_full_used)\n",
    "        y_full_pred = model.predict(X_raw)\n",
    "        if log_target:\n",
    "            y_full_pred = np.expm1(y_full_pred)\n",
    "        r2_full = r2_score(y_np, y_full_pred)\n",
    "results.append({\n",
    "            \"model\": name,\n",
    "            \"cv_rmse_mean\": float(np.mean(fold_rmses)),\n",
    "            \"cv_rmse_std\":  float(np.std(fold_rmses, ddof=0)),\n",
    "            \"r2_cv\":  float(r2_cv),\n",
    "            \"r2_full\": float(r2_full),\n",
    "        })\n",
    "return pd.DataFrame(results).sort_values(\"cv_rmse_mean\").reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241fcaa2-dcbb-4812-ade4-406d5d07dc2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use your *raw, unscaled* feature matrix (keep OHE; just skip StandardScaler on numerics)\n",
    "# If you only have X_final (scaled), rebuild X_raw by skipping the scaler step in your pipeline.\n",
    "X_raw = transformed_df\n",
    "results_raw = cv_eval_models_nonlinear(X_raw, y_final, cv=5, log_target=False, random_state=42)\n",
    "print(\"\\n=== Non-linear models on RAW target (raw features) ===\")\n",
    "print(results_raw.to_string(index=False))\n",
    "results_log = cv_eval_models_nonlinear(X_raw, y_final, cv=5, log_target=True, random_state=42)\n",
    "print(\"\\n=== Non-linear models on LOG target (raw features) ===\")\n",
    "print(results_log.to_string(index=False))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
