{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cfb7c95d-b537-40c7-818f-65e930a38b22",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jp/xgt314z12mgc2qh1wbn4mbx40000gn/T/ipykernel_14996/1767887551.py:7: DtypeWarning: Columns (36,40,81,87) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Ames_geo_original = pd.read_csv('Ames Real Estate Data.csv', index_col=0)\n"
     ]
    }
   ],
   "source": [
    "import os, time, math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from geopy.geocoders import Nominatim\n",
    "from geopy.extra.rate_limiter import RateLimiter\n",
    "\n",
    "Ames_geo_original = pd.read_csv('Ames Real Estate Data.csv', index_col=0)\n",
    "Ames_original = pd.read_csv('Ames_HousePrice.csv', index_col=0)\n",
    "\n",
    "Ames_geo = Ames_geo_original.copy()\n",
    "Ames = Ames_original.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5fb10797-4b06-432e-a043-f254f67880cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a full address string for geocoding\n",
    "Ames_geo[\"FullAddress\"] = (\n",
    "    Ames_geo[\"PA-Nmbr\"].fillna(\"\").astype(str).str.strip() + \" \" +\n",
    "    Ames_geo[\"PA-Strt\"].fillna(\"\").str.strip() + \" \" +\n",
    "    Ames_geo[\"PA-StSfx\"].fillna(\"\").str.strip() + \" \" +\n",
    "    Ames_geo[\"PA-PostD\"].fillna(\"\").str.strip() + \", Ames, IA\"\n",
    ").str.replace(\"  \", \" \").str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "64a7e669-7f73-4bc2-b27d-a6ee150d44b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original assessor rows: 22213\n",
      "Filtered to Ames rows: 2602\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1) Load the two datasets\n",
    "# Ames is your modeling dataset\n",
    "# Ames_geo is the assessor dataset\n",
    "# Assuming they are already loaded into Ames and Ames_geo\n",
    "\n",
    "# 2) Ensure keys are same dtype (PID ↔ MapRefNo)\n",
    "Ames[\"PID\"] = Ames[\"PID\"].astype(str)\n",
    "Ames_geo = Ames_geo.reset_index()\n",
    "Ames_geo[\"MapRefNo\"] = Ames_geo[\"MapRefNo\"].astype(str)\n",
    "\n",
    "# 3) Filter assessor data to only rows in Ames\n",
    "Ames_geo_filtered = Ames_geo[Ames_geo[\"MapRefNo\"].isin(Ames[\"PID\"])].copy()\n",
    "\n",
    "print(\"Original assessor rows:\", len(Ames_geo))\n",
    "print(\"Filtered to Ames rows:\", len(Ames_geo_filtered))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8f7b16b0-dfb6-4302-9d75-2c0fc73425cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ames PID raw (col): ['909176150', '905476230', '911128020', '535377150', '534177230']\n",
      "Geo MapRefNo raw (col): ['520400001', '520440010', '520440020', '520445001', '520445010']\n",
      "Overlap count: 2558\n",
      "Overlap sample: ['527146135', '906392120', '906475170', '904300150', '909253190', '534450090', '906382020', '528118090', '527357020', '916403230']\n",
      "Filtered geo rows: 2602\n",
      "      MapRefNo          Prop_Addr PA-Nmbr PA-PreD    PA-Strt PA-StSfx  \\\n",
      "113  526350040     3109 GROVE AVE  3109.0     NaN      GROVE      AVE   \n",
      "118  526351010   630 RIVER OAK DR   630.0     NaN  RIVER OAK       DR   \n",
      "120  526351030   620 RIVER OAK DR   620.0     NaN  RIVER OAK       DR   \n",
      "127  526351100     3010 GROVE AVE  3010.0     NaN      GROVE      AVE   \n",
      "272  526301100  3126 NORTHWOOD DR  3126.0     NaN  NORTHWOOD       DR   \n",
      "276  526302030  3115 NORTHWOOD DR  3115.0     NaN  NORTHWOOD       DR   \n",
      "277  526302040  3113 NORTHWOOD DR  3113.0     NaN  NORTHWOOD       DR   \n",
      "284  526302110   623 RIVER OAK DR   623.0     NaN  RIVER OAK       DR   \n",
      "285  526302120   627 RIVER OAK DR   627.0     NaN  RIVER OAK       DR   \n",
      "304  526352080   3006 KELLOGG AVE  3006.0     NaN    KELLOGG      AVE   \n",
      "\n",
      "     PA-PostD                       FullAddress  \n",
      "113       NaN     3109 GROVE AVE, Ames, IA, USA  \n",
      "118       NaN   630 RIVER OAK DR, Ames, IA, USA  \n",
      "120       NaN   620 RIVER OAK DR, Ames, IA, USA  \n",
      "127       NaN     3010 GROVE AVE, Ames, IA, USA  \n",
      "272       NaN  3126 NORTHWOOD DR, Ames, IA, USA  \n",
      "276       NaN  3115 NORTHWOOD DR, Ames, IA, USA  \n",
      "277       NaN  3113 NORTHWOOD DR, Ames, IA, USA  \n",
      "284       NaN   623 RIVER OAK DR, Ames, IA, USA  \n",
      "285       NaN   627 RIVER OAK DR, Ames, IA, USA  \n",
      "304       NaN   3006 KELLOGG AVE, Ames, IA, USA  \n"
     ]
    }
   ],
   "source": [
    "# 0) Make copies so we don't mutate originals\n",
    "ames = Ames.copy()\n",
    "geo  = Ames_geo.copy()\n",
    "\n",
    "# 1) Normalize join keys as *columns* (not index) → strings of ints with no decimals\n",
    "ames[\"PID_key\"] = pd.to_numeric(ames[\"PID\"], errors=\"coerce\").astype(\"Int64\").astype(str)\n",
    "geo[\"MapRefNo_key\"] = pd.to_numeric(geo[\"MapRefNo\"], errors=\"coerce\").astype(\"Int64\").astype(str)\n",
    "\n",
    "print(\"Ames PID raw (col):\", ames[\"PID\"].head(5).tolist())\n",
    "print(\"Geo MapRefNo raw (col):\", geo[\"MapRefNo\"].head(5).tolist())\n",
    "\n",
    "# 2) Quick overlap sanity check\n",
    "pid_set = set(ames[\"PID_key\"].dropna())\n",
    "geo_set = set(geo[\"MapRefNo_key\"].dropna())\n",
    "overlap = pid_set & geo_set\n",
    "print(\"Overlap count:\", len(overlap))\n",
    "print(\"Overlap sample:\", list(overlap)[:10])\n",
    "\n",
    "# 3) Filter geo to only PIDs present in Ames\n",
    "geo_filtered = geo[geo[\"MapRefNo_key\"].isin(pid_set)].copy()\n",
    "print(\"Filtered geo rows:\", len(geo_filtered))\n",
    "\n",
    "# 4) (Optional) build a clean full address string on the filtered geo for geocoding\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def _as_text(x):\n",
    "    \"\"\"Coerce any value to a clean string; handle NaN and floats like 4507.0 -> '4507'.\"\"\"\n",
    "    if pd.isna(x):\n",
    "        return \"\"\n",
    "    # numeric house numbers often come as floats with .0\n",
    "    if isinstance(x, (int, np.integer)):\n",
    "        return str(x)\n",
    "    if isinstance(x, float):\n",
    "        return str(int(x)) if x.is_integer() else str(x)\n",
    "    return str(x)\n",
    "\n",
    "def build_full_address(row):\n",
    "    # Pull parts and sanitize\n",
    "    num    = _as_text(row.get(\"PA-Nmbr\"))\n",
    "    pre    = _as_text(row.get(\"PA-PreD\"))\n",
    "    street = _as_text(row.get(\"PA-Strt\"))\n",
    "    sfx    = _as_text(row.get(\"PA-StSfx\"))\n",
    "    post   = _as_text(row.get(\"PA-PostD\"))\n",
    "    # Assemble, dropping empties / 'nan'\n",
    "    parts = [p.strip() for p in [num, pre, street, sfx, post] if p and p.strip().lower() != \"nan\"]\n",
    "\n",
    "    if not parts:\n",
    "        # fall back to Prop_Addr if structured parts are missing\n",
    "        prop = _as_text(row.get(\"Prop_Addr\")).strip()\n",
    "        if prop and prop.lower() != \"nan\":\n",
    "            parts = [prop]\n",
    "\n",
    "    return (\" \".join(parts) + \", Ames, IA, USA\") if parts else \"\"\n",
    "\n",
    "# --- Guard: only try to build if we actually have rows to process\n",
    "if len(geo_filtered) == 0:\n",
    "    print(\"geo_filtered is empty — double-check your PID ↔ MapRefNo key normalization and overlap.\")\n",
    "else:\n",
    "    geo_filtered = geo_filtered.copy()\n",
    "    geo_filtered[\"FullAddress\"] = geo_filtered.apply(build_full_address, axis=1)\n",
    "    print(geo_filtered[[\"MapRefNo\", \"Prop_Addr\", \"PA-Nmbr\", \"PA-PreD\", \"PA-Strt\", \"PA-StSfx\", \"PA-PostD\", \"FullAddress\"]].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ae35c953-6203-49c6-acab-0c3e1eb7db64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique addresses to geocode: 2,552\n",
      "Already cached: 0 | Still need: 2,552\n",
      "Processing in 7 chunk(s) of up to 400 addresses…\n",
      "[Chunk 1/7] 0..399 (size=400)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jp/xgt314z12mgc2qh1wbn4mbx40000gn/T/ipykernel_14996/862216844.py:153: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  cache = pd.concat([cache, tmp_df[[COL_ADDR, \"Lat\", \"Lon\"]]], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Flushed 50 new records | total cached=50\n",
      "  Flushed 100 new records | total cached=100\n",
      "  Flushed 150 new records | total cached=150\n",
      "  Flushed 200 new records | total cached=200\n",
      "  Flushed 250 new records | total cached=250\n",
      "  Flushed 300 new records | total cached=300\n",
      "  Flushed 350 new records | total cached=350\n",
      "  Flushed 400 new records | total cached=400\n",
      "  Chunk flushed | total cached=400\n",
      "  Chunk time: 497.0s | processed so far: 400\n",
      "[Chunk 2/7] 400..799 (size=400)\n",
      "  Flushed 50 new records | total cached=450\n",
      "  Flushed 100 new records | total cached=500\n",
      "  Flushed 150 new records | total cached=550\n",
      "  Flushed 200 new records | total cached=600\n",
      "  Flushed 250 new records | total cached=650\n",
      "  Flushed 300 new records | total cached=700\n",
      "  Flushed 350 new records | total cached=750\n",
      "  Flushed 400 new records | total cached=800\n",
      "  Chunk flushed | total cached=800\n",
      "  Chunk time: 506.2s | processed so far: 800\n",
      "[Chunk 3/7] 800..1199 (size=400)\n",
      "  Flushed 50 new records | total cached=850\n",
      "  Flushed 100 new records | total cached=900\n",
      "  Flushed 150 new records | total cached=950\n",
      "  Flushed 200 new records | total cached=1,000\n",
      "  Flushed 250 new records | total cached=1,050\n",
      "  Flushed 300 new records | total cached=1,100\n",
      "  Flushed 350 new records | total cached=1,150\n",
      "  Flushed 400 new records | total cached=1,200\n",
      "  Chunk flushed | total cached=1,200\n",
      "  Chunk time: 492.6s | processed so far: 1,200\n",
      "[Chunk 4/7] 1200..1599 (size=400)\n",
      "  Flushed 50 new records | total cached=1,250\n",
      "  Flushed 100 new records | total cached=1,300\n",
      "  Flushed 150 new records | total cached=1,350\n",
      "  Flushed 200 new records | total cached=1,400\n",
      "  Flushed 250 new records | total cached=1,450\n",
      "  Flushed 300 new records | total cached=1,500\n",
      "  Flushed 350 new records | total cached=1,550\n",
      "  Flushed 400 new records | total cached=1,600\n",
      "  Chunk flushed | total cached=1,600\n",
      "  Chunk time: 501.0s | processed so far: 1,600\n",
      "[Chunk 5/7] 1600..1999 (size=400)\n",
      "  Flushed 50 new records | total cached=1,650\n",
      "  Flushed 100 new records | total cached=1,700\n",
      "  Flushed 150 new records | total cached=1,750\n",
      "  Flushed 200 new records | total cached=1,800\n",
      "  Flushed 250 new records | total cached=1,850\n",
      "  Flushed 300 new records | total cached=1,900\n",
      "  Flushed 350 new records | total cached=1,950\n",
      "  Flushed 400 new records | total cached=2,000\n",
      "  Chunk flushed | total cached=2,000\n",
      "  Chunk time: 498.3s | processed so far: 2,000\n",
      "[Chunk 6/7] 2000..2399 (size=400)\n",
      "  Flushed 50 new records | total cached=2,050\n",
      "  Flushed 100 new records | total cached=2,100\n",
      "  Flushed 150 new records | total cached=2,150\n",
      "  Flushed 200 new records | total cached=2,200\n",
      "  Flushed 250 new records | total cached=2,250\n",
      "  Flushed 300 new records | total cached=2,300\n",
      "  Flushed 350 new records | total cached=2,350\n",
      "  Flushed 400 new records | total cached=2,400\n",
      "  Chunk flushed | total cached=2,400\n",
      "  Chunk time: 487.2s | processed so far: 2,400\n",
      "[Chunk 7/7] 2400..2551 (size=152)\n",
      "  Flushed 50 new records | total cached=2,450\n",
      "  Flushed 100 new records | total cached=2,500\n",
      "  Flushed 150 new records | total cached=2,550\n",
      "  Chunk flushed | total cached=2,552\n",
      "  Chunk time: 186.8s | processed so far: 2,552\n",
      "Saved unique results -> geocode_results.csv (2,552 rows)\n",
      "Failures saved -> geocode_failures.csv (28 rows)\n",
      "Merged Lat/Lon -> ames_with_latlon.csv (2,602 rows)\n",
      "Geocoding success rate on all rows: 98.7%\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Overnight Geocoding Script (Nominatim / OpenStreetMap)\n",
    "# Requirements:\n",
    "#   pip install geopy\n",
    "#\n",
    "# Input in memory:\n",
    "  # ames_with_addr : pd.DataFrame with columns [\"PID\", \"FullAddress\"]\n",
    "#\n",
    "# Outputs on disk:\n",
    "#   geocode_cache.csv     (running cache for unique addresses)\n",
    "#   geocode_results.csv   (final unique address -> Lat, Lon)\n",
    "#   ames_with_latlon.csv  (merge Lat/Lon back to your rows by FullAddress)\n",
    "#   geocode_failures.csv  (addresses that failed geocoding)\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "import pandas as pd\n",
    "from geopy.geocoders import Nominatim\n",
    "from geopy.extra.rate_limiter import RateLimiter\n",
    "\n",
    "# -----------------------------\n",
    "# 0) CONFIG — EDIT IF NEEDED\n",
    "# -----------------------------\n",
    "COL_PID       = \"MapRefNo\"          # your unique row identifier\n",
    "COL_ADDR      = \"FullAddress\"  # full address string, e.g., \"123 MAIN ST, Ames, IA, USA\"\n",
    "\n",
    "CACHE_PATH    = \"geocode_cache.csv\"     # resume file for (FullAddress, Lat, Lon)\n",
    "RESULTS_PATH  = \"geocode_results.csv\"   # final unique address results\n",
    "MERGED_PATH   = \"ames_with_latlon.csv\"  # merged back onto ames_with_addr\n",
    "FAIL_PATH     = \"geocode_failures.csv\"  # addresses that never resolved\n",
    "\n",
    "CHUNK_SIZE    = 400     # addresses per chunk (tune if you like)\n",
    "FLUSH_EVERY   = 50      # write cache every N lookups for safety\n",
    "MIN_DELAY_SEC = 1.2     # Nominatim policy ~1 req/sec (be polite)\n",
    "MAX_RETRIES   = 3       # per-address retries\n",
    "\n",
    "# A descriptive user_agent is required by Nominatim. Add a contact email if possible.\n",
    "USER_AGENT    = \"ames-housing-geocoder (contact: you@example.com)\"\n",
    "\n",
    "# -----------------------------\n",
    "# 1) SANITY CHECKS\n",
    "# -----------------------------\n",
    "\n",
    "ames_with_addr = geo_filtered.copy()\n",
    "\n",
    "if \"ames_with_addr\" not in globals():\n",
    "    raise RuntimeError(\"Expected a DataFrame named `ames_with_addr` in memory.\")\n",
    "\n",
    "missing_cols = {COL_PID, COL_ADDR} - set(ames_with_addr.columns)\n",
    "if missing_cols:\n",
    "    raise RuntimeError(f\"`ames_with_addr` is missing required columns: {missing_cols}\")\n",
    "\n",
    "if ames_with_addr.empty:\n",
    "    raise RuntimeError(\"`ames_with_addr` is empty.\")\n",
    "\n",
    "# Drop obviously empty/NA addresses, and normalize to string\n",
    "src = ames_with_addr.copy()\n",
    "src[COL_ADDR] = src[COL_ADDR].astype(str).str.strip()\n",
    "src = src[src[COL_ADDR].str.len() > 0].copy()\n",
    "\n",
    "# -----------------------------\n",
    "# 2) DEDUPE ADDRESSES\n",
    "# -----------------------------\n",
    "addr_unique = (\n",
    "    src[[COL_ADDR]]\n",
    "    .drop_duplicates()\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "print(f\"Unique addresses to geocode: {len(addr_unique):,}\")\n",
    "\n",
    "# -----------------------------\n",
    "# 3) LOAD / PREP CACHE\n",
    "# -----------------------------\n",
    "if os.path.exists(CACHE_PATH):\n",
    "    cache = pd.read_csv(CACHE_PATH)\n",
    "    # Normalize cache schema and types\n",
    "    cache = cache.rename(columns={\n",
    "        COL_ADDR: COL_ADDR,\n",
    "        \"lat\": \"Lat\",\n",
    "        \"lon\": \"Lon\"\n",
    "    })\n",
    "    # Keep only needed cols; drop duplicates\n",
    "    keep_cols = [c for c in [COL_ADDR, \"Lat\", \"Lon\"] if c in cache.columns]\n",
    "    cache = cache[keep_cols].drop_duplicates(subset=[COL_ADDR], keep=\"last\")\n",
    "else:\n",
    "    cache = pd.DataFrame(columns=[COL_ADDR, \"Lat\", \"Lon\"])\n",
    "\n",
    "# Fast lookup set\n",
    "cached_ok = set(cache[COL_ADDR].dropna().unique())\n",
    "need_mask = ~addr_unique[COL_ADDR].isin(cached_ok)\n",
    "to_geocode = addr_unique.loc[need_mask].reset_index(drop=True)\n",
    "\n",
    "print(f\"Already cached: {len(cached_ok):,} | Still need: {len(to_geocode):,}\")\n",
    "\n",
    "# -----------------------------\n",
    "# 4) GEOCODER + RATE LIMITER\n",
    "# -----------------------------\n",
    "geolocator = Nominatim(user_agent=USER_AGENT, timeout=10)  # base timeout per request\n",
    "\n",
    "# Wrap with RateLimiter for pacing + retries\n",
    "geocode_raw = geolocator.geocode\n",
    "geocode = RateLimiter(\n",
    "    geocode_raw,\n",
    "    min_delay_seconds=MIN_DELAY_SEC,   # spacing between calls\n",
    "    max_retries=MAX_RETRIES,\n",
    "    error_wait_seconds=5,              # wait between retry rounds\n",
    "    swallow_exceptions=False           # raise so we can log failures\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# 5) PROCESS IN CHUNKS\n",
    "# -----------------------------\n",
    "\n",
    "def geocode_one(addr: str):\n",
    "    \"\"\"Return dict with address, Lat, Lon (or NaNs) and error flag.\"\"\"\n",
    "    try:\n",
    "        loc = geocode(addr, exactly_one=True, addressdetails=False)\n",
    "        if loc is None:\n",
    "            return {COL_ADDR: addr, \"Lat\": float(\"nan\"), \"Lon\": float(\"nan\"), \"ok\": False}\n",
    "        return {COL_ADDR: addr, \"Lat\": loc.latitude, \"Lon\": loc.longitude, \"ok\": True}\n",
    "    except Exception as e:\n",
    "        # You can log e if desired\n",
    "        return {COL_ADDR: addr, \"Lat\": float(\"nan\"), \"Lon\": float(\"nan\"), \"ok\": False}\n",
    "\n",
    "# Work chunk-by-chunk\n",
    "results_new = []  # only for this run (not the pre-existing cache)\n",
    "\n",
    "if len(to_geocode) > 0:\n",
    "    n_chunks = math.ceil(len(to_geocode) / CHUNK_SIZE)\n",
    "    print(f\"Processing in {n_chunks} chunk(s) of up to {CHUNK_SIZE} addresses…\")\n",
    "\n",
    "    processed = 0\n",
    "    for ci in range(n_chunks):\n",
    "        a = ci * CHUNK_SIZE\n",
    "        b = min((ci + 1) * CHUNK_SIZE, len(to_geocode))\n",
    "        chunk = to_geocode.iloc[a:b].reset_index(drop=True)\n",
    "\n",
    "        print(f\"[Chunk {ci+1}/{n_chunks}] {a}..{b-1} (size={len(chunk)})\")\n",
    "        t0 = time.time()\n",
    "\n",
    "        for i, addr in enumerate(chunk[COL_ADDR].tolist(), start=1):\n",
    "            rec = geocode_one(addr)\n",
    "            results_new.append(rec)\n",
    "            processed += 1\n",
    "\n",
    "            # Periodically flush to disk (append to cache)\n",
    "            if (len(results_new) % FLUSH_EVERY) == 0:\n",
    "                tmp_df = pd.DataFrame(results_new)\n",
    "                # Merge new into cache (dedupe by address; keep last)\n",
    "                cache = pd.concat([cache, tmp_df[[COL_ADDR, \"Lat\", \"Lon\"]]], ignore_index=True)\n",
    "                cache = cache.drop_duplicates(subset=[COL_ADDR], keep=\"last\")\n",
    "                cache.to_csv(CACHE_PATH, index=False)\n",
    "                print(f\"  Flushed {len(results_new)} new records | total cached={len(cache):,}\")\n",
    "\n",
    "        # End of chunk — flush what’s left\n",
    "        if results_new:\n",
    "            tmp_df = pd.DataFrame(results_new)\n",
    "            cache = pd.concat([cache, tmp_df[[COL_ADDR, \"Lat\", \"Lon\"]]], ignore_index=True)\n",
    "            cache = cache.drop_duplicates(subset=[COL_ADDR], keep=\"last\")\n",
    "            cache.to_csv(CACHE_PATH, index=False)\n",
    "            print(f\"  Chunk flushed | total cached={len(cache):,}\")\n",
    "            results_new = []\n",
    "\n",
    "        t1 = time.time()\n",
    "        print(f\"  Chunk time: {t1 - t0:.1f}s | processed so far: {processed:,}\")\n",
    "else:\n",
    "    print(\"Nothing new to geocode — using cache only.\")\n",
    "\n",
    "# -----------------------------\n",
    "# 6) FINALIZE UNIQUE RESULTS\n",
    "# -----------------------------\n",
    "unique_results = cache.drop_duplicates(subset=[COL_ADDR], keep=\"last\").reset_index(drop=True)\n",
    "unique_results.to_csv(RESULTS_PATH, index=False)\n",
    "print(f\"Saved unique results -> {RESULTS_PATH} ({len(unique_results):,} rows)\")\n",
    "\n",
    "# Failures list\n",
    "fail_df = unique_results[unique_results[\"Lat\"].isna()].copy()\n",
    "fail_df.to_csv(FAIL_PATH, index=False)\n",
    "print(f\"Failures saved -> {FAIL_PATH} ({len(fail_df):,} rows)\")\n",
    "\n",
    "# -----------------------------\n",
    "# 7) MERGE BACK TO YOUR ROWS\n",
    "# -----------------------------\n",
    "merged = src.merge(unique_results, on=COL_ADDR, how=\"left\")\n",
    "merged.to_csv(MERGED_PATH, index=False)\n",
    "print(f\"Merged Lat/Lon -> {MERGED_PATH} ({len(merged):,} rows)\")\n",
    "\n",
    "# Quick success rate\n",
    "succ_rate = merged[\"Lat\"].notna().mean()\n",
    "print(f\"Geocoding success rate on all rows: {succ_rate:.1%}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
